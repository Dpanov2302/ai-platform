from fastapi import FastAPI, File, UploadFile, HTTPException
from fastapi.responses import StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
import onnxruntime as ort
import numpy as np
import io
import cv2
import logging

from coco_classes import COCO_CLASSES

app = FastAPI()
logger = logging.getLogger("uvicorn.error")

# CORS (Ð¼Ð¾Ð¶Ð½Ð¾ Ð¾Ð³Ñ€Ð°Ð½Ð¸Ñ‡Ð¸Ñ‚ÑŒ)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Ð“Ð»Ð¾Ð±Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ð¿ÐµÑ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ðµ
sessions = {}


@app.on_event("startup")
def load_models():
    global sessions
    try:
        logger.info("ÐÐ°Ñ‡Ð°Ð»Ð¾ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ ONNX")
        sessions["efficientnet"] = ort.InferenceSession("models/efficientnet-lite4-11-qdq.onnx")
        sessions["yolov5"] = ort.InferenceSession("models/yolov5n.onnx")
        logger.info("ÐœÐ¾Ð´ÐµÐ»Ð¸ Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ñ‹")
    except Exception as e:
        raise RuntimeError(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸ ONNX Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹: {e}")


# ðŸ§  ÐšÐ»Ð°ÑÑÐ¸Ñ„Ð¸ÐºÐ°Ñ†Ð¸Ñ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ
@app.post("/classify")
async def classify_image(file: UploadFile = File(...)):
    try:
        image_bytes = await file.read()
        img_np = np.frombuffer(image_bytes, np.uint8)
        img = cv2.imdecode(img_np, cv2.IMREAD_COLOR)
        img = cv2.resize(img, (224, 224))
        img = img.astype(np.float32) / 255.0
        img = img[np.newaxis, :]  # NHWC: (1, 224, 224, 3)

        input_tensor = sessions["efficientnet"].get_inputs()[0]
        print(f"INPUT NAME: {input_tensor.name}")
        print(f"INPUT SHAPE: {input_tensor.shape}")
        print(f"INPUT TYPE: {input_tensor.type}")

        input_name = input_tensor.name
        output = sessions["efficientnet"].run(None, {input_name: img})[0]
        prediction = int(np.argmax(output))
        logger.info(f"classify/prediction: {prediction}")
        return {"class_id": prediction}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ÐžÑˆÐ¸Ð±ÐºÐ° ÐºÐ»Ð°ÑÑÐ¸Ñ„Ð¸ÐºÐ°Ñ†Ð¸Ð¸: {e}")


# ðŸ§  Ð”ÐµÑ‚ÐµÐºÑ†Ð¸Ñ Ð¾Ð±ÑŠÐµÐºÑ‚Ð¾Ð² YOLOv5
@app.post("/detect-image-annotated")
async def detect_image_annotated(file: UploadFile = File(...)):
    try:
        image_bytes = await file.read()
        img_np = np.frombuffer(image_bytes, np.uint8)
        img = cv2.imdecode(img_np, cv2.IMREAD_COLOR)

        orig_h, orig_w = img.shape[:2]

        # ÐŸÐ¾Ð´Ð³Ð¾Ñ‚Ð¾Ð²ÐºÐ° Ð´Ð»Ñ YOLO
        img_resized = cv2.resize(img, (640, 640))
        img_input = img_resized.astype(np.float16) / 255.0
        img_input = np.transpose(img_input, (2, 0, 1))[np.newaxis, :]

        input_name = sessions["yolov5"].get_inputs()[0].name
        output = sessions["yolov5"].run(None, {input_name: img_input})[0][0]

        boxes = []
        confidences = []
        class_ids = []

        for row in output:
            conf = row[4]
            if conf > 0.4:
                class_id = int(np.argmax(row[5:]))
                xc, yc, w, h = row[:4]

                scale_x = orig_w / 640
                scale_y = orig_h / 640
                x1 = int((xc - w / 2) * scale_x)
                y1 = int((yc - h / 2) * scale_y)
                box_w = int(w * scale_x)
                box_h = int(h * scale_y)

                boxes.append([x1, y1, box_w, box_h])
                confidences.append(float(conf))
                class_ids.append(class_id)

        # ÐŸÑ€Ð¸Ð¼ÐµÐ½Ð¸Ñ‚ÑŒ NMS
        indices = cv2.dnn.NMSBoxes(boxes, confidences, score_threshold=0.4, nms_threshold=0.5)

        if isinstance(indices, (list, tuple)) and len(indices) == 0:
            indices = []
        else:
            indices = indices.flatten()

        for i in indices:
            x, y, w, h = boxes[i]
            label = COCO_CLASSES[class_ids[i]] if class_ids[i] < len(COCO_CLASSES) else f"id {class_ids[i]}"
            conf = confidences[i]

            text = f"{label} ({conf:.2f})"
            cv2.rectangle(img, (x, y), (x + w, y + h), (0, 0, 255), 2)
            cv2.putText(img, text, (x, max(y - 10, 10)),
                        cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 0, 255), 2)

        # Ð’Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÐ¼ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ðµ
        _, img_encoded = cv2.imencode(".jpg", img)
        return StreamingResponse(io.BytesIO(img_encoded.tobytes()), media_type="image/jpeg")

    except Exception as e:
        import traceback
        print("ERROR in detect_image_annotated:", traceback.format_exc())
        raise HTTPException(status_code=500, detail=f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð¸Ð½Ñ„ÐµÑ€ÐµÐ½ÑÐµ Ð¸ Ð¾Ñ‚Ñ€Ð¸ÑÐ¾Ð²ÐºÐµ: {e}")


@app.get("/")
def root():
    return {"message": "ONNX model server is running"}
